{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import graphinglib as gl\n",
    "import pyregion\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy\n",
    "import dill\n",
    "from functools import partial\n",
    "\n",
    "from src.tools.statistics.advanced_stats import autocorrelation_function, autocorrelation_function_2d, \\\n",
    "                                                get_autocorrelation_function_2d_contour\n",
    "from src.hdu.maps.map import Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plottables(data: np.ndarray, region_radius: float) -> list:\n",
    "    sorted_data = data[np.argsort(data[:,0])]\n",
    "    scatter = gl.Scatter(\n",
    "        sorted_data[:,0],\n",
    "        sorted_data[:,1],\n",
    "        marker_size=5,\n",
    "        face_color=\"black\",\n",
    "    )\n",
    "\n",
    "    # scatter.add_errorbars(\n",
    "    #     y_error=sorted_data[:,2],\n",
    "    #     errorbars_line_width=0.25,\n",
    "    #     cap_width=0,\n",
    "    # )\n",
    "\n",
    "    return [scatter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbulence = Map.load(\"summer_2023/gaussian_fitting/maps/computed_data_selective/turbulence.fits\")\n",
    "\n",
    "regions = [\n",
    "    (\"Global region\", None, 50),\n",
    "    (\"Diffuse region\", pyregion.open(\"summer_2023/gaussian_fitting/regions/region_1.reg\"), 20),\n",
    "    (\"Central region\", pyregion.open(\"summer_2023/gaussian_fitting/regions/region_2.reg\"), 10),\n",
    "    (\"Filament region\", pyregion.open(\"summer_2023/gaussian_fitting/regions/region_3.reg\"), 6)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation functions of all regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = []\n",
    "\n",
    "for name, region, region_radius in regions:\n",
    "    data = autocorrelation_function(turbulence.get_masked_region(region).data, method=\"Boily\")\n",
    "\n",
    "    fig = gl.Figure(\n",
    "        title=name,\n",
    "        x_label=\"Lag\",\n",
    "        y_label=\"Autocorrelation function\",\n",
    "        x_lim=(0, region_radius*1.2),\n",
    "        y_lim=(-1, 1),\n",
    "    )\n",
    "    plottables = get_plottables(data, region_radius)\n",
    "    cropped_scatter = plottables[0].create_slice_x(0, region_radius*1.2)\n",
    "    # fig.y_lim = np.min(cropped_scatter.y_data)-0.2, np.max(cropped_scatter.y_data)+0.3\n",
    "    fig.add_elements(*plottables)\n",
    "    figs.append(fig)\n",
    "\n",
    "multifig = gl.MultiFigure.from_grid(figs, (2,2), (14, 9))\n",
    "multifig.save(\"figures/sh158/advanced_stats/autocorrelation/autocorrelation_functions_new_2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = autocorrelation_function_2d(turbulence.data, method=\"Boily\")\n",
    "fig = gl.Figure(\n",
    "    x_lim=(np.min(data[:,0]), np.max(data[:,0])),\n",
    "    y_lim=(-np.max(data[:,1]), np.max(data[:,1])),\n",
    ")\n",
    "cont = get_autocorrelation_function_2d_contour(data)\n",
    "\n",
    "fig.add_elements(cont)\n",
    "fig.save(\"test_new.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "])\n",
    "print(autocorrelation_function(d))\n",
    "\n",
    "d_mean = np.array([\n",
    "    [-4,-3,-2],\n",
    "    [-1, 0, 1],\n",
    "    [ 2, 3, 4]\n",
    "])\n",
    "print(np.sum(d_mean**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-4*-3+-3*-2+-4*-1+-3*0+-2*1+-1*0+0*1+-1*2+0*3+1*4+2*3+3*4 # lag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40/60*np.sqrt(9)/(12)**(3/2) / (np.sqrt(9)/(9)**(3/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "test_array = np.random.random((50,50))\n",
    "test_array[:,1] = np.NAN\n",
    "# print(test_array - np.nanmean(test_array))\n",
    "acr = autocorrelation_function(test_array, \"Boily\")\n",
    "# print(acr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = test_array - np.nanmean(test_array)\n",
    "\n",
    "# Create arrays that will be useful for computing distances\n",
    "x, y = np.arange(new_array.shape[1]), np.arange(new_array.shape[0])\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "dists_and_multiplications = []\n",
    "\n",
    "for y in range(new_array.shape[0]):\n",
    "    for x in range(new_array.shape[1]):\n",
    "        if not np.isnan(new_array[y, x]):\n",
    "            multiplication = new_array[y, x] * new_array\n",
    "            dists = np.sqrt((x-xx)**2 + (y-yy)**2)\n",
    "            # The multiplication's result is linked to the pixels' distance\n",
    "            dists_and_multiplications.append(np.stack((dists, multiplication), axis=2))\n",
    "\n",
    "unique_dists = {}\n",
    "for pix in dists_and_multiplications:\n",
    "    for row in pix:\n",
    "        for dist, multiplication in row:\n",
    "            unique_dists[dist] = (unique_dists.get(dist, []) + [multiplication])\n",
    "\n",
    "mean_dists = []\n",
    "norm = np.nanmean(new_array**2)\n",
    "for dist, vals in unique_dists.items():\n",
    "    mean_dists.append([dist, np.nanmean(vals) / norm])\n",
    "arr = np.array(mean_dists)\n",
    "arr = arr[np.argsort(arr[:,0])]\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(acr[:,:2], arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
