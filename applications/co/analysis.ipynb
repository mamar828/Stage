{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinetic temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kinetic temperature is obtained with the 12CO amplitude :\n",
    "$$T_{kin}=T_{x}=\\frac{5.532}{\\ln\\left(1+\\left(\\frac{T_A}{5.532}+0.151\\right)^{-1}\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hdu.maps.map import Map\n",
    "from src.hdu.cubes.cube_co import CubeCO\n",
    "from src.hdu.tesseract import Tesseract\n",
    "from src.hdu.maps.grouped_maps import GroupedMaps\n",
    "from src.hdu.maps.convenient_funcs import get_kinetic_temperature\n",
    "from src.coordinates.ds9_coords import DS9Coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kinetic_temperature(prefix: str):\n",
    "    GroupedMaps([(\n",
    "        \"kinetic_temperature\", [\n",
    "            get_kinetic_temperature(amp) for amp in Tesseract.load(\n",
    "                f\"data/Loop4_co/{prefix}/12co/object_filtered.fits\"\n",
    "            ).to_grouped_maps().amplitude\n",
    "        ]\n",
    "    )]).save(f\"data/Loop4_co/{prefix}/12co/kinetic_temperature.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_kinetic_temperature(\"N1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_kinetic_temperature(\"N2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_kinetic_temperature(\"N4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_kinetic_temperature(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column density is obtained using the following equation (Interstellar And Intergalactic Medium, Barbara Ryden and Richard W. Pogge):\n",
    "\\begin{align*}\n",
    "    N_0\\left(^{13}\\text{CO}\\right)=\\int_{-\\infty}^\\infty T_A\\left(^{13}\\text{CO}\\right)\\cdot0.8\\cdot\\frac{g_0}{g_1A_{10}}\\cdot\\frac{\\pi k\\nu^2}{hc^3}\\left[\\left(\\frac1{\\exp\\left(\\frac{h\\nu}{kT_x}\\right)-1}-\\frac1{\\exp\\left(\\frac{h\\nu}{kT_{rad}}\\right)-1}\\right)\\left(1-\\exp\\left(-\\frac{h\\nu}{kT_x}\\right)\\right)\\right]^{-1}\n",
    "\\end{align*}\n",
    "knowing that\n",
    "\\begin{align*}\n",
    "    \\int_{-\\infty}^\\infty T_A\\left(^{13}\\text{CO}\\right)dv&=2T_A\\sigma\\sqrt{\\frac\\pi2}\\text{erf}\\left(\\frac{\\infty}{\\sqrt2\\sigma}\\right)\\\\\n",
    "    &=2T_A\\sigma\\sqrt{\\frac\\pi2}\\\\\n",
    "\\end{align*}\n",
    "as\n",
    "$$\\lim_{x\\rightarrow\\infty}\\text{erf}(x)=1$$\n",
    "we obtain\n",
    "$$N_0\\left(^{13}\\text{CO}\\right)=2T_A\\sigma\\sqrt{\\frac\\pi2}\\cdot0.8\\cdot\\frac{g_0}{g_1A_{10}}\\cdot\\frac{\\pi k\\nu^2}{hc^3}\\left[\\left(\\frac1{\\exp\\left(\\frac{h\\nu}{kT_x}\\right)-1}-\\frac1{\\exp\\left(\\frac{h\\nu}{kT_{rad}}\\right)-1}\\right)\\left(1-\\exp\\left(-\\frac{h\\nu}{kT_x}\\right)\\right)\\right]^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import graphinglib as gl\n",
    "import importlib\n",
    "import scipy\n",
    "\n",
    "import src.hdu.maps.convenient_funcs\n",
    "importlib.reload(src.hdu.maps.convenient_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_component_column_density(prefix: str):\n",
    "    cube_12co = CubeCO.load(f\"data/Loop4_co/{prefix}/12co/Loop4{prefix}_wcs.fits\")\n",
    "    cube_13co = CubeCO.load(f\"data/Loop4_co/{prefix}/13co/Loop4{prefix}_13co.fits\")\n",
    "    maps_12co = Tesseract.load(f\"data/Loop4_co/{prefix}/12co/object_filtered.fits\").to_grouped_maps()\n",
    "    maps_13co = Tesseract.load(f\"data/Loop4_co/{prefix}/13co/tesseract.fits\").to_grouped_maps()\n",
    "\n",
    "    # The right gaussians first need to be selected\n",
    "    # This solution is for single component 13co maps\n",
    "    assert len(maps_13co.mean) == 1\n",
    "    mean_12co = np.stack([m.get_reprojection_on(maps_13co.mean[0]).data for m in maps_12co.mean], axis=0)\n",
    "    offset_12 = sum([int(line.split(\" \")[5][:-1]) if line[12:33] == \"was sliced at channel\" else 0 \n",
    "                     for line in maps_12co.mean[0].header[\"COMMENT\"]])\n",
    "    offset_13 = sum([int(line.split(\" \")[5][:-1]) if line[13:34] == \"was sliced at channel\" else 0 \n",
    "                     for line in maps_13co.mean[0].header[\"COMMENT\"]])\n",
    "\n",
    "    speed_convert_12 = np.vectorize(cube_12co.header.get_value)\n",
    "    speed_convert_13 = np.vectorize(cube_13co.header.get_value)\n",
    "    # Compute the diff between the centroid of every gaussian\n",
    "    diff_array = np.abs(speed_convert_12(mean_12co + offset_12)\n",
    "                      - speed_convert_13(maps_13co.mean[0].data + offset_13))\n",
    "    nan_mask = np.isnan(diff_array)     # Apply a nan mask to allow proper argmin use\n",
    "    diff_array[nan_mask] = 2**15-1      # Remove nans\n",
    "    min_mask = np.argmin(diff_array, axis=0)\n",
    "    filter_gaussians = lambda arr: np.take_along_axis(arr, min_mask[np.newaxis, ...], axis=0).squeeze()\n",
    "\n",
    "    amp_12co_val = np.stack(\n",
    "        [m.get_reprojection_on(maps_13co.mean[0]).data for m in maps_12co.amplitude], axis=0\n",
    "    )\n",
    "    amp_12co_unc = np.stack(\n",
    "        [m.get_reprojection_on(maps_13co.mean[0]).uncertainties for m in maps_12co.amplitude], axis=0\n",
    "    )\n",
    "\n",
    "    amplitude_correction_factor_13co = 0.43\n",
    "    src.hdu.maps.convenient_funcs.get_13co_column_density(\n",
    "        stddev_13co=maps_13co.stddev[0]*np.abs(cube_13co.header[\"CDELT3\"]/1000),\n",
    "        antenna_temperature_13co=maps_13co.amplitude[0]/amplitude_correction_factor_13co,\n",
    "        antenna_temperature_12co=Map(filter_gaussians(amp_12co_val), filter_gaussians(amp_12co_unc))\n",
    "    ).save(f\"data/Loop4_co/{prefix}/13co/{prefix}_column_density.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_single_component_column_density(\"N1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_single_component_column_density(\"N2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_single_component_column_density(\"N4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop4p multiple components\n",
    "import scipy.optimize\n",
    "\n",
    "cube_12co = CubeCO.load(\"data/Loop4_co/p/12co/Loop4p_wcs.fits\")\n",
    "cube_13co = CubeCO.load(\"data/Loop4_co/p/13co/Loop4p_13co.fits\")\n",
    "maps_12co = Tesseract.load(\"data/Loop4_co/p/12co/object_filtered.fits\").to_grouped_maps()\n",
    "maps_13co = Tesseract.load(\"data/Loop4_co/p/13co/object_filtered.fits\").to_grouped_maps()\n",
    "\n",
    "mean_12co = np.stack([m.get_reprojection_on(maps_13co.mean[0]).data for m in maps_12co.mean], axis=0)\n",
    "mean_13co = np.stack([m.data for m in maps_13co.mean], axis=0)\n",
    "ampl_12co_val = np.stack([m.get_reprojection_on(maps_13co.amplitude[0]).data for m in maps_12co.amplitude], axis=0)\n",
    "ampl_12co_unc = np.stack([m.get_reprojection_on(maps_13co.amplitude[0]).uncertainties for m in maps_12co.amplitude],\n",
    "                         axis=0)\n",
    "\n",
    "ordered_stddev_13co = np.full([*mean_13co.shape, 2], np.NAN)\n",
    "ordered_amplitude_13co = np.full([*mean_13co.shape, 2], np.NAN)\n",
    "ordered_amplitude_12co = np.full([*mean_13co.shape, 2], np.NAN)\n",
    "\n",
    "speed_convert_12 = np.vectorize(cube_12co.header.get_value)\n",
    "speed_convert_13 = np.vectorize(cube_13co.header.get_value)\n",
    "\n",
    "def minimize(target: np.ndarray, ref: np.ndarray):\n",
    "    \"\"\" \n",
    "    Minimizes the distance between two groups of points and gives the matching indices.\n",
    "    \"\"\"\n",
    "    # Create a cost matrix where the element at position (i, j) represents the difference between list1[i] and list2[j]\n",
    "    cost_matrix = np.abs(np.subtract.outer(target[~np.isnan(target)], ref[~np.isnan(ref)]))\n",
    "    # Use linear_sum_assignment to find the optimal assignment\n",
    "    row_indices, col_indices = scipy.optimize.linear_sum_assignment(cost_matrix)\n",
    "    # Create a list of tuples representing the pairs\n",
    "    pairs = list(zip(row_indices, col_indices))\n",
    "    # Check if the pairs are close enough, otherwise the pair is considered invalid\n",
    "    velocity_upper_limit = 100\n",
    "    valid_pairs = []\n",
    "    for pair in pairs:\n",
    "        if np.abs(target[pair[0]] - ref[pair[1]]) < velocity_upper_limit:\n",
    "            valid_pairs.append(pair)\n",
    "    return valid_pairs\n",
    "\n",
    "for y in range(mean_13co.shape[1]):\n",
    "    for x in range(mean_13co.shape[2]):\n",
    "        if not np.isnan(mean_13co[0,y,x]):\n",
    "            matches = minimize(speed_convert_13(mean_13co[:,y,x]+400), speed_convert_12(mean_12co[:,y,x]+500))\n",
    "            for match in matches:\n",
    "                ordered_stddev_13co[match[0],y,x] = [\n",
    "                    maps_13co.stddev[match[0]].data[y,x],\n",
    "                    maps_13co.stddev[match[0]].uncertainties[y,x]\n",
    "                ]\n",
    "                ordered_amplitude_13co[match[0],y,x] = [\n",
    "                    maps_13co.amplitude[match[0]].data[y,x],\n",
    "                    maps_13co.amplitude[match[0]].uncertainties[y,x]\n",
    "                ]\n",
    "                ordered_amplitude_12co[match[0],y,x] = [\n",
    "                    ampl_12co_val[match[1],y,x],\n",
    "                    ampl_12co_unc[match[1],y,x]\n",
    "                ]\n",
    "\n",
    "amplitude_correction_factor_13co = 0.43\n",
    "column_densities = []\n",
    "for i in range(mean_13co.shape[0]):\n",
    "    column_densities.append(\n",
    "        src.hdu.maps.convenient_funcs.get_13co_column_density(\n",
    "            stddev_13co=Map(\n",
    "                data=ordered_stddev_13co[i,:,:,0],\n",
    "                uncertainties=ordered_stddev_13co[i,:,:,1],\n",
    "                header=maps_13co.mean[0].header,\n",
    "            ) * np.abs(cube_13co.header[\"CDELT3\"]/1000),\n",
    "            antenna_temperature_13co=Map(\n",
    "                data=ordered_amplitude_13co[i,:,:,0],\n",
    "                uncertainties=ordered_amplitude_13co[i,:,:,1],\n",
    "                header=maps_13co.mean[0].header,\n",
    "            ) / amplitude_correction_factor_13co,\n",
    "            antenna_temperature_12co=Map(\n",
    "                data=ordered_amplitude_12co[i,:,:,0],\n",
    "                uncertainties=ordered_amplitude_12co[i,:,:,1],\n",
    "                header=maps_13co.mean[0].header,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "GroupedMaps([(\"column_density\", column_densities)]).save(\"data/Loop4_co/p/13co/p_column_density.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2 column density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_h2_column_density(\n",
    "        prefix: str,\n",
    "):\n",
    "    tess = Tesseract.load(f\"data/Loop4_co/{prefix}/12co/object_filtered.fits\")\n",
    "    cube = CubeCO.load(f\"data/Loop4_co/{prefix}/12co/Loop4{prefix}_wcs.fits\")\n",
    "    X_CO = 2.5e20\n",
    "    gm = tess.to_grouped_maps()\n",
    "    n_h2 = []\n",
    "    for amplitude, stddev in zip(gm.amplitude, gm.stddev):\n",
    "        n_h2.append(\n",
    "            src.hdu.maps.convenient_funcs.integrate_gaussian(\n",
    "                amplitude_map=amplitude,\n",
    "                stddev_map=stddev * np.abs(cube.header[\"CDELT3\"]) / 1000\n",
    "            ) * X_CO\n",
    "        )\n",
    "    GroupedMaps([(\"H2_column_density\", n_h2)]).save(f\"data/Loop4_co/{prefix}/12co/{prefix}_H2_column_density.fits\")\n",
    "    Map(\n",
    "        data=np.nansum([m.data for m in n_h2], axis=0),\n",
    "        uncertainties=np.nansum([m.uncertainties for m in n_h2], axis=0),\n",
    "        header=n_h2[0].header,\n",
    "    ).num_to_nan().save(f\"data/Loop4_co/{prefix}/12co/{prefix}_H2_column_density_total.fits\")\n",
    "\n",
    "def calculate_h2_column_density_with_13co(\n",
    "        prefix: str,\n",
    "):\n",
    "    if prefix == \"p\":\n",
    "        column_densities = GroupedMaps.load(\"data/Loop4_co/p/13co/p_column_density.fits\").column_density\n",
    "        (2.2e6 * Map(\n",
    "            data=np.nansum([m.data for m in column_densities], axis=0),\n",
    "            uncertainties=np.nansum([m.uncertainties for m in column_densities], axis=0),\n",
    "            header=column_densities[0].header,\n",
    "        ).num_to_nan()).save(f\"data/Loop4_co/{prefix}/13co/{prefix}_H2_column_density_total_13co.fits\")\n",
    "        \n",
    "    else:\n",
    "        column_density = Map.load(f\"data/Loop4_co/{prefix}/13co/{prefix}_column_density.fits\")\n",
    "        (2.2e6 * column_density).save(f\"data/Loop4_co/{prefix}/13co/{prefix}_H2_column_density_total_13co.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_h2_column_density(\"N1\")\n",
    "calculate_h2_column_density(\"N2\")\n",
    "calculate_h2_column_density(\"N4\")\n",
    "calculate_h2_column_density(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_h2_column_density_with_13co(\"N1\")\n",
    "calculate_h2_column_density_with_13co(\"N2\")\n",
    "calculate_h2_column_density_with_13co(\"N4\")\n",
    "calculate_h2_column_density_with_13co(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cloud_mass(\n",
    "        prefix: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Gives the cloud's mass in kg.\n",
    "    \"\"\"\n",
    "    alpha = 30 / 3600 * (2*np.pi)/360 \n",
    "    if prefix in [\"N1\", \"p\"]:       # These two clouds were binned 2x2 whuch results in \n",
    "        alpha *= 2\n",
    "    D = 370 * scipy.constants.parsec * 100\n",
    "    mu = 2.4\n",
    "    m_H = scipy.constants.proton_mass + scipy.constants.electron_mass\n",
    "\n",
    "    n_h2 = Map.load(f\"data/Loop4_co/{prefix}/12co/{prefix}_H2_column_density_total.fits\")\n",
    "    sum_n_h2 = np.array([\n",
    "        np.nansum(n_h2.data),\n",
    "        np.nansum(n_h2.uncertainties),\n",
    "    ])\n",
    "    M = (alpha * D)**2 * mu * m_H * sum_n_h2\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = lambda x: f\"({x[0]:.5e} ± {x[1]:.5e})\"\n",
    "for cloud in [\"N1\", \"N2\", \"N4\", \"p\"]:\n",
    "    m = calculate_cloud_mass(cloud)\n",
    "    print(f\"Cloud {cloud:2}: M(H2)={fm(m):27} kg, M(CO)={fm(3e-6 * m):27} kg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
